{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-9a9541bbef15>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from capsnet import CapsNet\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/')\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_random_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = './tmp/model.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, restore = False, n_epochs = 50):\n",
    "    init = tf.global_variables_initializer()\t\n",
    "\n",
    "    n_iter_train_per_epoch = mnist.train.num_examples // batch_size\n",
    "    n_iter_valid_per_epoch = mnist.validation.num_examples // batch_size\n",
    "\n",
    "    best_loss_val = np.infty\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(\"output\", sess.graph)\n",
    "\n",
    "        if restore and tf.train.checkpoint_exists('checkpoint_file'):\n",
    "            saver.restore(sess, checkpoint_file)\n",
    "        else:\n",
    "            init.run()\n",
    "\n",
    "        print('\\n\\nRunning CapsNet ...\\n')\n",
    "        for epoch in range(n_epochs):\n",
    "            loss_train_ep = []\n",
    "            acc_train_ep  = []\n",
    "            for it in range(1, n_iter_train_per_epoch + 1):\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "\n",
    "                _, loss_batch_train, acc_batch_train = sess.run(\n",
    "                                [model.train_op, model.batch_loss, model.accuracy],\n",
    "                                feed_dict = {model.X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                                                model.y: y_batch,\n",
    "                                                model.reconstruction: True})\n",
    "\n",
    "                print(\"\\rIter: {}/{} [{:.1f}%] loss : {:.5f}\".format(\n",
    "                    it, n_iter_train_per_epoch, 100.0 * it / n_iter_train_per_epoch, loss_batch_train), end=\"\")\n",
    "\n",
    "                loss_train_ep.append(loss_batch_train)\n",
    "                acc_train_ep.append(acc_batch_train)\n",
    "            loss_train = np.mean(loss_train_ep)\n",
    "            acc_train = np.mean(acc_train_ep)\n",
    "            \n",
    "            loss_val_ep = []\n",
    "            acc_val_ep  = []\n",
    "\n",
    "            for it in range(1, n_iter_valid_per_epoch + 1):\n",
    "                X_batch, y_batch = mnist.validation.next_batch(batch_size)\n",
    "                loss_batch_val, acc_batch_val = sess.run(\n",
    "                                [model.batch_loss, model.accuracy],\n",
    "                                feed_dict = {model.X_cropped: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                                                model.y: y_batch})\n",
    "\n",
    "                loss_val_ep.append(loss_batch_val)\n",
    "                acc_val_ep.append(acc_batch_val)\n",
    "\n",
    "                print(\"\\rValidation {}/{} {:.1f}%\".format(it, n_iter_valid_per_epoch, 100.0 * it / n_iter_valid_per_epoch), end=\" \"*30)\n",
    "\n",
    "            loss_val = np.mean(loss_val_ep)\n",
    "            acc_val  = np.mean(acc_val_ep)\n",
    "\n",
    "            print(\"\\repoch: {} loss_train: {:.5f}, loss_val: {:.5f}, train_acc: {:.4f}%, valid_acc: {:.4f}% {}\".format(\n",
    "                epoch + 1, loss_train, loss_val, acc_train * 100.0, acc_val * 100.0, \"(improved)\" if loss_val < best_loss_val else \"\"))\n",
    "\n",
    "            if loss_val < best_loss_val:\n",
    "                best_loss_val = loss_val\n",
    "            saver.save(sess, checkpoint_file)\n",
    "            \n",
    "\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/josechavez_ie/src/repos/capsnet-tensorflow/capsules.py:30: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /home/josechavez_ie/src/repos/capsnet-tensorflow/capsnet.py:109: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model = CapsNet(rounds = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running CapsNet ...\n",
      "\n",
      "epoch: 1 loss_train: 0.12319, loss_val: 0.03748, train_acc: 88.5398%, valid_acc: 98.4375% (improved)\n",
      "epoch: 2 loss_train: 0.03925, loss_val: 0.02751, train_acc: 98.2772%, valid_acc: 99.0385% (improved)\n",
      "epoch: 3 loss_train: 0.02950, loss_val: 0.02278, train_acc: 98.9001%, valid_acc: 99.2388% (improved)\n",
      "epoch: 4 loss_train: 0.02515, loss_val: 0.01911, train_acc: 99.0876%, valid_acc: 99.4591% (improved)\n",
      "epoch: 5 loss_train: 0.02150, loss_val: 0.01973, train_acc: 99.2552%, valid_acc: 99.1386% \n",
      "epoch: 6 loss_train: 0.01957, loss_val: 0.01646, train_acc: 99.3681%, valid_acc: 99.4191% (improved)\n",
      "epoch: 7 loss_train: 0.01764, loss_val: 0.01486, train_acc: 99.4701%, valid_acc: 99.4391% (improved)\n",
      "epoch: 8 loss_train: 0.01633, loss_val: 0.01384, train_acc: 99.5374%, valid_acc: 99.5192% (improved)\n",
      "epoch: 9 loss_train: 0.01565, loss_val: 0.01567, train_acc: 99.5629%, valid_acc: 99.4792% \n",
      "epoch: 10 loss_train: 0.01458, loss_val: 0.01318, train_acc: 99.6503%, valid_acc: 99.6194% (improved)\n",
      "epoch: 11 loss_train: 0.01381, loss_val: 0.01266, train_acc: 99.6449%, valid_acc: 99.5192% (improved)\n",
      "epoch: 12 loss_train: 0.01324, loss_val: 0.01158, train_acc: 99.6540%, valid_acc: 99.6394% (improved)\n",
      "epoch: 13 loss_train: 0.01183, loss_val: 0.01160, train_acc: 99.7669%, valid_acc: 99.4792% \n",
      "epoch: 14 loss_train: 0.01194, loss_val: 0.01146, train_acc: 99.7450%, valid_acc: 99.4191% (improved)\n",
      "epoch: 15 loss_train: 0.01107, loss_val: 0.01071, train_acc: 99.8088%, valid_acc: 99.5994% (improved)\n",
      "epoch: 16 loss_train: 0.01092, loss_val: 0.01101, train_acc: 99.7942%, valid_acc: 99.5793% \n",
      "epoch: 17 loss_train: 0.01047, loss_val: 0.01121, train_acc: 99.8379%, valid_acc: 99.4992% \n",
      "epoch: 18 loss_train: 0.01029, loss_val: 0.01024, train_acc: 99.8252%, valid_acc: 99.5793% (improved)\n",
      "epoch: 19 loss_train: 0.00967, loss_val: 0.01055, train_acc: 99.8434%, valid_acc: 99.4792% \n",
      "epoch: 20 loss_train: 0.00939, loss_val: 0.00994, train_acc: 99.8671%, valid_acc: 99.5994% (improved)\n",
      "epoch: 21 loss_train: 0.00909, loss_val: 0.00989, train_acc: 99.8834%, valid_acc: 99.5393% (improved)\n",
      "epoch: 22 loss_train: 0.00872, loss_val: 0.01031, train_acc: 99.8798%, valid_acc: 99.5393% \n",
      "epoch: 23 loss_train: 0.00857, loss_val: 0.00943, train_acc: 99.8998%, valid_acc: 99.5793% (improved)\n",
      "epoch: 24 loss_train: 0.00804, loss_val: 0.00960, train_acc: 99.9126%, valid_acc: 99.6194% \n",
      "epoch: 25 loss_train: 0.00800, loss_val: 0.00987, train_acc: 99.9144%, valid_acc: 99.4591% \n",
      "epoch: 26 loss_train: 0.00779, loss_val: 0.01000, train_acc: 99.9217%, valid_acc: 99.5994% \n",
      "epoch: 27 loss_train: 0.00780, loss_val: 0.01005, train_acc: 99.9253%, valid_acc: 99.5793% \n",
      "epoch: 28 loss_train: 0.00748, loss_val: 0.00874, train_acc: 99.9435%, valid_acc: 99.5793% (improved)\n",
      "epoch: 29 loss_train: 0.00716, loss_val: 0.00904, train_acc: 99.9472%, valid_acc: 99.6194% \n",
      "epoch: 30 loss_train: 0.00725, loss_val: 0.00889, train_acc: 99.9326%, valid_acc: 99.5994% \n",
      "epoch: 31 loss_train: 0.00700, loss_val: 0.00899, train_acc: 99.9363%, valid_acc: 99.5994% \n",
      "epoch: 32 loss_train: 0.00691, loss_val: 0.00847, train_acc: 99.9545%, valid_acc: 99.5393% (improved)\n",
      "epoch: 33 loss_train: 0.00688, loss_val: 0.00883, train_acc: 99.9472%, valid_acc: 99.5994% \n",
      "epoch: 34 loss_train: 0.00669, loss_val: 0.00986, train_acc: 99.9545%, valid_acc: 99.4391% \n",
      "epoch: 35 loss_train: 0.00635, loss_val: 0.00844, train_acc: 99.9654%, valid_acc: 99.6394% (improved)\n",
      "epoch: 36 loss_train: 0.00639, loss_val: 0.00878, train_acc: 99.9672%, valid_acc: 99.5593% \n",
      "epoch: 37 loss_train: 0.00644, loss_val: 0.00867, train_acc: 99.9545%, valid_acc: 99.5793% \n",
      "epoch: 38 loss_train: 0.00630, loss_val: 0.00876, train_acc: 99.9581%, valid_acc: 99.5793% \n",
      "epoch: 39 loss_train: 0.00624, loss_val: 0.00905, train_acc: 99.9599%, valid_acc: 99.4992% \n",
      "epoch: 40 loss_train: 0.00605, loss_val: 0.00831, train_acc: 99.9781%, valid_acc: 99.6194% (improved)\n",
      "epoch: 41 loss_train: 0.00591, loss_val: 0.00822, train_acc: 99.9818%, valid_acc: 99.5793% (improved)\n",
      "epoch: 42 loss_train: 0.00589, loss_val: 0.00842, train_acc: 99.9727%, valid_acc: 99.5593% \n",
      "epoch: 43 loss_train: 0.00588, loss_val: 0.00831, train_acc: 99.9690%, valid_acc: 99.5593% \n",
      "epoch: 44 loss_train: 0.00580, loss_val: 0.00833, train_acc: 99.9672%, valid_acc: 99.6194% \n",
      "epoch: 45 loss_train: 0.00559, loss_val: 0.00854, train_acc: 99.9818%, valid_acc: 99.5593% \n",
      "epoch: 46 loss_train: 0.00571, loss_val: 0.00798, train_acc: 99.9763%, valid_acc: 99.6595% (improved)\n",
      "epoch: 47 loss_train: 0.00568, loss_val: 0.00803, train_acc: 99.9690%, valid_acc: 99.6394% \n",
      "epoch: 48 loss_train: 0.00553, loss_val: 0.00907, train_acc: 99.9800%, valid_acc: 99.4591% \n",
      "epoch: 49 loss_train: 0.00554, loss_val: 0.00772, train_acc: 99.9873%, valid_acc: 99.5994% (improved)\n",
      "epoch: 50 loss_train: 0.00537, loss_val: 0.00884, train_acc: 99.9800%, valid_acc: 99.5593% \n",
      "epoch: 51 loss_train: 0.00535, loss_val: 0.00823, train_acc: 99.9781%, valid_acc: 99.5793% \n",
      "epoch: 52 loss_train: 0.00529, loss_val: 0.00802, train_acc: 99.9745%, valid_acc: 99.5393% \n",
      "epoch: 53 loss_train: 0.00527, loss_val: 0.00775, train_acc: 99.9836%, valid_acc: 99.5593% \n",
      "epoch: 54 loss_train: 0.00504, loss_val: 0.00824, train_acc: 99.9873%, valid_acc: 99.5593% \n",
      "epoch: 55 loss_train: 0.00507, loss_val: 0.00842, train_acc: 99.9854%, valid_acc: 99.5393% \n",
      "epoch: 56 loss_train: 0.00530, loss_val: 0.00866, train_acc: 99.9836%, valid_acc: 99.4591% \n",
      "epoch: 57 loss_train: 0.00511, loss_val: 0.00793, train_acc: 99.9836%, valid_acc: 99.5192% \n",
      "epoch: 58 loss_train: 0.00496, loss_val: 0.00836, train_acc: 99.9909%, valid_acc: 99.5793% \n",
      "epoch: 59 loss_train: 0.00498, loss_val: 0.00826, train_acc: 99.9909%, valid_acc: 99.5393% \n",
      "epoch: 60 loss_train: 0.00492, loss_val: 0.00768, train_acc: 99.9836%, valid_acc: 99.5994% (improved)\n",
      "epoch: 61 loss_train: 0.00491, loss_val: 0.00819, train_acc: 99.9909%, valid_acc: 99.5593% \n",
      "epoch: 62 loss_train: 0.00481, loss_val: 0.00781, train_acc: 99.9927%, valid_acc: 99.4992% \n",
      "epoch: 63 loss_train: 0.00479, loss_val: 0.00805, train_acc: 99.9945%, valid_acc: 99.4792% \n",
      "epoch: 64 loss_train: 0.00486, loss_val: 0.00817, train_acc: 99.9909%, valid_acc: 99.5393% \n",
      "epoch: 65 loss_train: 0.00489, loss_val: 0.00760, train_acc: 99.9854%, valid_acc: 99.5593% (improved)\n",
      "epoch: 66 loss_train: 0.00459, loss_val: 0.00787, train_acc: 99.9927%, valid_acc: 99.5593% \n",
      "epoch: 67 loss_train: 0.00464, loss_val: 0.00759, train_acc: 99.9873%, valid_acc: 99.5994% (improved)\n",
      "epoch: 68 loss_train: 0.00472, loss_val: 0.00778, train_acc: 99.9891%, valid_acc: 99.6595% \n",
      "epoch: 69 loss_train: 0.00460, loss_val: 0.00898, train_acc: 99.9927%, valid_acc: 99.4391% \n",
      "epoch: 70 loss_train: 0.00459, loss_val: 0.00777, train_acc: 99.9945%, valid_acc: 99.5793% \n",
      "epoch: 71 loss_train: 0.00456, loss_val: 0.00786, train_acc: 99.9891%, valid_acc: 99.6394% \n",
      "epoch: 72 loss_train: 0.00457, loss_val: 0.00863, train_acc: 99.9945%, valid_acc: 99.4191% \n",
      "epoch: 73 loss_train: 0.00463, loss_val: 0.00847, train_acc: 99.9909%, valid_acc: 99.4992% \n",
      "epoch: 74 loss_train: 0.00449, loss_val: 0.00782, train_acc: 99.9964%, valid_acc: 99.5593% \n",
      "epoch: 75 loss_train: 0.00440, loss_val: 0.00793, train_acc: 99.9927%, valid_acc: 99.4992% \n",
      "epoch: 76 loss_train: 0.00435, loss_val: 0.00782, train_acc: 99.9964%, valid_acc: 99.5192% \n",
      "epoch: 77 loss_train: 0.00438, loss_val: 0.00759, train_acc: 99.9964%, valid_acc: 99.5192% \n",
      "epoch: 78 loss_train: 0.00438, loss_val: 0.00759, train_acc: 99.9964%, valid_acc: 99.6194% \n",
      "epoch: 79 loss_train: 0.00442, loss_val: 0.00831, train_acc: 99.9964%, valid_acc: 99.4792% \n",
      "epoch: 80 loss_train: 0.00428, loss_val: 0.00699, train_acc: 99.9982%, valid_acc: 99.5994% (improved)\n",
      "epoch: 81 loss_train: 0.00420, loss_val: 0.00796, train_acc: 99.9982%, valid_acc: 99.5393% \n",
      "epoch: 82 loss_train: 0.00435, loss_val: 0.00759, train_acc: 99.9982%, valid_acc: 99.6194% \n",
      "epoch: 83 loss_train: 0.00433, loss_val: 0.00735, train_acc: 99.9927%, valid_acc: 99.5192% \n",
      "epoch: 84 loss_train: 0.00425, loss_val: 0.00754, train_acc: 99.9982%, valid_acc: 99.5393% \n",
      "epoch: 85 loss_train: 0.00424, loss_val: 0.00756, train_acc: 99.9982%, valid_acc: 99.5393% \n",
      "epoch: 86 loss_train: 0.00424, loss_val: 0.00704, train_acc: 99.9982%, valid_acc: 99.7196% \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 87 loss_train: 0.00411, loss_val: 0.00788, train_acc: 99.9964%, valid_acc: 99.5393% \n",
      "epoch: 88 loss_train: 0.00418, loss_val: 0.00756, train_acc: 99.9927%, valid_acc: 99.5793% \n",
      "epoch: 89 loss_train: 0.00420, loss_val: 0.00798, train_acc: 99.9964%, valid_acc: 99.4391% \n",
      "epoch: 90 loss_train: 0.00406, loss_val: 0.00798, train_acc: 100.0000%, valid_acc: 99.4391% \n",
      "epoch: 91 loss_train: 0.00419, loss_val: 0.00764, train_acc: 99.9927%, valid_acc: 99.5994% \n",
      "epoch: 92 loss_train: 0.00406, loss_val: 0.00734, train_acc: 99.9982%, valid_acc: 99.6795% \n",
      "epoch: 93 loss_train: 0.00401, loss_val: 0.00784, train_acc: 99.9982%, valid_acc: 99.5192% \n",
      "epoch: 94 loss_train: 0.00416, loss_val: 0.00787, train_acc: 99.9964%, valid_acc: 99.5192% \n",
      "epoch: 95 loss_train: 0.00404, loss_val: 0.00704, train_acc: 99.9982%, valid_acc: 99.5994% \n",
      "epoch: 96 loss_train: 0.00397, loss_val: 0.00755, train_acc: 99.9982%, valid_acc: 99.6194% \n",
      "epoch: 97 loss_train: 0.00397, loss_val: 0.00756, train_acc: 99.9964%, valid_acc: 99.5793% \n",
      "epoch: 98 loss_train: 0.00407, loss_val: 0.00788, train_acc: 99.9982%, valid_acc: 99.5593% \n",
      "epoch: 99 loss_train: 0.00410, loss_val: 0.00722, train_acc: 99.9964%, valid_acc: 99.6194% \n",
      "epoch: 100 loss_train: 0.00390, loss_val: 0.00770, train_acc: 99.9982%, valid_acc: 99.5192% \n",
      "epoch: 101 loss_train: 0.00395, loss_val: 0.00743, train_acc: 99.9982%, valid_acc: 99.5593% \n",
      "epoch: 102 loss_train: 0.00386, loss_val: 0.00699, train_acc: 99.9982%, valid_acc: 99.6595% (improved)\n",
      "epoch: 103 loss_train: 0.00390, loss_val: 0.00730, train_acc: 99.9982%, valid_acc: 99.5593% \n",
      "epoch: 104 loss_train: 0.00416, loss_val: 0.00779, train_acc: 99.9909%, valid_acc: 99.5593% \n",
      "epoch: 105 loss_train: 0.00410, loss_val: 0.00775, train_acc: 99.9909%, valid_acc: 99.5593% \n",
      "epoch: 106 loss_train: 0.00382, loss_val: 0.00770, train_acc: 99.9982%, valid_acc: 99.5793% \n",
      "epoch: 107 loss_train: 0.00378, loss_val: 0.00673, train_acc: 100.0000%, valid_acc: 99.6995% (improved)\n",
      "epoch: 108 loss_train: 0.00383, loss_val: 0.00719, train_acc: 99.9964%, valid_acc: 99.4792% \n",
      "epoch: 109 loss_train: 0.00371, loss_val: 0.00777, train_acc: 99.9982%, valid_acc: 99.4992% \n",
      "epoch: 110 loss_train: 0.00397, loss_val: 0.00783, train_acc: 99.9964%, valid_acc: 99.4191% \n",
      "epoch: 111 loss_train: 0.00386, loss_val: 0.00730, train_acc: 100.0000%, valid_acc: 99.5793% \n",
      "epoch: 112 loss_train: 0.00380, loss_val: 0.00681, train_acc: 100.0000%, valid_acc: 99.5793% \n",
      "epoch: 113 loss_train: 0.00394, loss_val: 0.00751, train_acc: 99.9945%, valid_acc: 99.4992% \n",
      "epoch: 114 loss_train: 0.00375, loss_val: 0.00629, train_acc: 99.9982%, valid_acc: 99.6795% (improved)\n",
      "epoch: 115 loss_train: 0.00373, loss_val: 0.00782, train_acc: 100.0000%, valid_acc: 99.4591% \n",
      "epoch: 116 loss_train: 0.00375, loss_val: 0.00750, train_acc: 100.0000%, valid_acc: 99.5393% \n",
      "epoch: 117 loss_train: 0.00381, loss_val: 0.00713, train_acc: 99.9982%, valid_acc: 99.5593% \n",
      "epoch: 118 loss_train: 0.00387, loss_val: 0.00752, train_acc: 99.9964%, valid_acc: 99.5994% \n",
      "epoch: 119 loss_train: 0.00384, loss_val: 0.00711, train_acc: 99.9982%, valid_acc: 99.5593% \n",
      "epoch: 120 loss_train: 0.00374, loss_val: 0.00719, train_acc: 100.0000%, valid_acc: 99.5994% \n",
      "epoch: 121 loss_train: 0.00363, loss_val: 0.00677, train_acc: 100.0000%, valid_acc: 99.6194% \n",
      "epoch: 122 loss_train: 0.00359, loss_val: 0.00726, train_acc: 100.0000%, valid_acc: 99.5793% \n",
      "epoch: 123 loss_train: 0.00360, loss_val: 0.00733, train_acc: 99.9982%, valid_acc: 99.4992% \n",
      "epoch: 124 loss_train: 0.00387, loss_val: 0.00723, train_acc: 99.9982%, valid_acc: 99.6595% \n",
      "epoch: 125 loss_train: 0.00367, loss_val: 0.00730, train_acc: 99.9982%, valid_acc: 99.5192% \n",
      "epoch: 126 loss_train: 0.00377, loss_val: 0.00774, train_acc: 99.9964%, valid_acc: 99.3990% \n",
      "epoch: 127 loss_train: 0.00376, loss_val: 0.00649, train_acc: 99.9964%, valid_acc: 99.6394% \n",
      "epoch: 128 loss_train: 0.00361, loss_val: 0.00716, train_acc: 100.0000%, valid_acc: 99.5994% \n",
      "epoch: 129 loss_train: 0.00364, loss_val: 0.00753, train_acc: 99.9964%, valid_acc: 99.5192% \n",
      "epoch: 130 loss_train: 0.00359, loss_val: 0.00737, train_acc: 99.9982%, valid_acc: 99.5793% \n",
      "epoch: 131 loss_train: 0.00362, loss_val: 0.00718, train_acc: 99.9982%, valid_acc: 99.5793% \n",
      "epoch: 132 loss_train: 0.00374, loss_val: 0.00682, train_acc: 100.0000%, valid_acc: 99.6595% \n",
      "epoch: 133 loss_train: 0.00356, loss_val: 0.00743, train_acc: 99.9982%, valid_acc: 99.6995% \n",
      "epoch: 134 loss_train: 0.00352, loss_val: 0.00690, train_acc: 100.0000%, valid_acc: 99.6995% \n",
      "epoch: 135 loss_train: 0.00361, loss_val: 0.00759, train_acc: 99.9982%, valid_acc: 99.5994% \n",
      "epoch: 136 loss_train: 0.00360, loss_val: 0.00710, train_acc: 99.9982%, valid_acc: 99.5593% \n",
      "epoch: 137 loss_train: 0.00356, loss_val: 0.00679, train_acc: 100.0000%, valid_acc: 99.5793% \n",
      "epoch: 138 loss_train: 0.00358, loss_val: 0.00814, train_acc: 100.0000%, valid_acc: 99.4591% \n",
      "epoch: 139 loss_train: 0.00361, loss_val: 0.00673, train_acc: 100.0000%, valid_acc: 99.6595% \n",
      "epoch: 140 loss_train: 0.00351, loss_val: 0.00793, train_acc: 99.9982%, valid_acc: 99.4591% \n",
      "epoch: 141 loss_train: 0.00350, loss_val: 0.00710, train_acc: 100.0000%, valid_acc: 99.5393% \n",
      "epoch: 142 loss_train: 0.00378, loss_val: 0.00790, train_acc: 99.9927%, valid_acc: 99.5192% \n",
      "epoch: 143 loss_train: 0.00361, loss_val: 0.00697, train_acc: 100.0000%, valid_acc: 99.5793% \n",
      "epoch: 144 loss_train: 0.00343, loss_val: 0.00672, train_acc: 100.0000%, valid_acc: 99.6194% \n",
      "epoch: 145 loss_train: 0.00340, loss_val: 0.00724, train_acc: 100.0000%, valid_acc: 99.5793% \n",
      "epoch: 146 loss_train: 0.00341, loss_val: 0.00750, train_acc: 100.0000%, valid_acc: 99.3990% \n",
      "epoch: 147 loss_train: 0.00349, loss_val: 0.00688, train_acc: 100.0000%, valid_acc: 99.5994% \n",
      "epoch: 148 loss_train: 0.00365, loss_val: 0.00733, train_acc: 99.9982%, valid_acc: 99.5593% \n",
      "epoch: 149 loss_train: 0.00346, loss_val: 0.00674, train_acc: 100.0000%, valid_acc: 99.6995% \n",
      "epoch: 150 loss_train: 0.00341, loss_val: 0.00682, train_acc: 100.0000%, valid_acc: 99.6194% \n",
      "epoch: 151 loss_train: 0.00367, loss_val: 0.00760, train_acc: 99.9982%, valid_acc: 99.5393% \n",
      "epoch: 152 loss_train: 0.00347, loss_val: 0.00683, train_acc: 100.0000%, valid_acc: 99.6394% \n",
      "epoch: 153 loss_train: 0.00333, loss_val: 0.00782, train_acc: 100.0000%, valid_acc: 99.4992% \n",
      "epoch: 154 loss_train: 0.00334, loss_val: 0.00715, train_acc: 100.0000%, valid_acc: 99.5994% \n",
      "epoch: 155 loss_train: 0.00350, loss_val: 0.00705, train_acc: 99.9982%, valid_acc: 99.5793% \n",
      "epoch: 156 loss_train: 0.00362, loss_val: 0.00672, train_acc: 99.9982%, valid_acc: 99.5994% \n",
      "epoch: 157 loss_train: 0.00343, loss_val: 0.00700, train_acc: 100.0000%, valid_acc: 99.5593% \n",
      "epoch: 158 loss_train: 0.00331, loss_val: 0.00730, train_acc: 100.0000%, valid_acc: 99.5192% \n",
      "epoch: 159 loss_train: 0.00335, loss_val: 0.00732, train_acc: 100.0000%, valid_acc: 99.5593% \n",
      "epoch: 160 loss_train: 0.00331, loss_val: 0.00688, train_acc: 100.0000%, valid_acc: 99.5793% \n",
      "epoch: 161 loss_train: 0.00336, loss_val: 0.00702, train_acc: 100.0000%, valid_acc: 99.4792% \n",
      "epoch: 162 loss_train: 0.00337, loss_val: 0.00727, train_acc: 100.0000%, valid_acc: 99.5192% \n",
      "epoch: 163 loss_train: 0.00339, loss_val: 0.00736, train_acc: 100.0000%, valid_acc: 99.5593% \n",
      "epoch: 164 loss_train: 0.00353, loss_val: 0.00717, train_acc: 100.0000%, valid_acc: 99.4792% \n",
      "epoch: 165 loss_train: 0.00341, loss_val: 0.00688, train_acc: 100.0000%, valid_acc: 99.5393% \n",
      "epoch: 166 loss_train: 0.00335, loss_val: 0.00723, train_acc: 99.9982%, valid_acc: 99.4792% \n",
      "epoch: 167 loss_train: 0.00330, loss_val: 0.00645, train_acc: 100.0000%, valid_acc: 99.6795% \n",
      "epoch: 168 loss_train: 0.00345, loss_val: 0.00747, train_acc: 100.0000%, valid_acc: 99.5593% \n",
      "epoch: 169 loss_train: 0.00338, loss_val: 0.00749, train_acc: 100.0000%, valid_acc: 99.5393% \n",
      "epoch: 170 loss_train: 0.00322, loss_val: 0.00675, train_acc: 100.0000%, valid_acc: 99.6394% \n",
      "epoch: 171 loss_train: 0.00330, loss_val: 0.00702, train_acc: 100.0000%, valid_acc: 99.6394% \n",
      "epoch: 172 loss_train: 0.00323, loss_val: 0.00756, train_acc: 99.9982%, valid_acc: 99.3790% \n",
      "epoch: 173 loss_train: 0.00325, loss_val: 0.00716, train_acc: 100.0000%, valid_acc: 99.4992% \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 174 loss_train: 0.00342, loss_val: 0.00753, train_acc: 100.0000%, valid_acc: 99.5393% \n",
      "epoch: 175 loss_train: 0.00343, loss_val: 0.00678, train_acc: 100.0000%, valid_acc: 99.5593% \n",
      "epoch: 176 loss_train: 0.00326, loss_val: 0.00707, train_acc: 100.0000%, valid_acc: 99.5793% \n",
      "epoch: 177 loss_train: 0.00331, loss_val: 0.00670, train_acc: 100.0000%, valid_acc: 99.5994% \n",
      "epoch: 178 loss_train: 0.00341, loss_val: 0.00705, train_acc: 99.9964%, valid_acc: 99.5793% \n",
      "epoch: 179 loss_train: 0.00326, loss_val: 0.00725, train_acc: 100.0000%, valid_acc: 99.4992% \n",
      "epoch: 180 loss_train: 0.00319, loss_val: 0.00698, train_acc: 100.0000%, valid_acc: 99.5393% \n",
      "epoch: 181 loss_train: 0.00323, loss_val: 0.00727, train_acc: 100.0000%, valid_acc: 99.5192% \n",
      "epoch: 182 loss_train: 0.00324, loss_val: 0.00694, train_acc: 100.0000%, valid_acc: 99.5994% \n",
      "epoch: 183 loss_train: 0.00329, loss_val: 0.00790, train_acc: 100.0000%, valid_acc: 99.4391% \n",
      "epoch: 184 loss_train: 0.00356, loss_val: 0.00823, train_acc: 99.9927%, valid_acc: 99.5393% \n",
      "epoch: 185 loss_train: 0.00327, loss_val: 0.00726, train_acc: 100.0000%, valid_acc: 99.5192% \n",
      "epoch: 186 loss_train: 0.00323, loss_val: 0.00749, train_acc: 99.9982%, valid_acc: 99.4391% \n",
      "epoch: 187 loss_train: 0.00323, loss_val: 0.00665, train_acc: 100.0000%, valid_acc: 99.5793% \n",
      "epoch: 188 loss_train: 0.00317, loss_val: 0.00712, train_acc: 100.0000%, valid_acc: 99.5793% \n",
      "epoch: 189 loss_train: 0.00313, loss_val: 0.00692, train_acc: 100.0000%, valid_acc: 99.5393% \n",
      "epoch: 190 loss_train: 0.00346, loss_val: 0.00775, train_acc: 100.0000%, valid_acc: 99.3590% \n",
      "epoch: 191 loss_train: 0.00321, loss_val: 0.00659, train_acc: 100.0000%, valid_acc: 99.6795% \n",
      "epoch: 192 loss_train: 0.00319, loss_val: 0.00734, train_acc: 100.0000%, valid_acc: 99.4391% \n",
      "epoch: 193 loss_train: 0.00321, loss_val: 0.00679, train_acc: 100.0000%, valid_acc: 99.6394% \n",
      "epoch: 194 loss_train: 0.00316, loss_val: 0.00769, train_acc: 100.0000%, valid_acc: 99.3389% \n",
      "epoch: 195 loss_train: 0.00316, loss_val: 0.00654, train_acc: 100.0000%, valid_acc: 99.5393% \n",
      "epoch: 196 loss_train: 0.00316, loss_val: 0.00695, train_acc: 100.0000%, valid_acc: 99.5793% \n",
      "epoch: 197 loss_train: 0.00314, loss_val: 0.00776, train_acc: 100.0000%, valid_acc: 99.4792% \n",
      "epoch: 198 loss_train: 0.00347, loss_val: 0.00822, train_acc: 99.9982%, valid_acc: 99.3990% \n",
      "epoch: 199 loss_train: 0.00348, loss_val: 0.00672, train_acc: 99.9982%, valid_acc: 99.6194% \n",
      "epoch: 200 loss_train: 0.00319, loss_val: 0.00784, train_acc: 100.0000%, valid_acc: 99.3990% \n"
     ]
    }
   ],
   "source": [
    "train(model, False, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    batch_size = 100\n",
    "\n",
    "    n_iter_test_per_epoch = mnist.test.num_examples // batch_size\n",
    "\n",
    "    loss_test_ep = []\n",
    "    acc_test_ep  = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.import_meta_graph(checkpoint_file +'.meta')\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('tmp/'))\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        for it in range(1, n_iter_test_per_epoch + 1):\n",
    "            X_batch, y_batch = mnist.test.next_batch(batch_size)\n",
    "            loss_batch_test, acc_batch_test = sess.run(\n",
    "                [model.batch_loss, model.accuracy],\n",
    "                    feed_dict = {model.X_cropped: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                                model.y: y_batch,\n",
    "                                model.reconstruction: False})\n",
    "\n",
    "            loss_test_ep.append(loss_batch_test)\n",
    "            acc_test_ep.append(acc_batch_test)\n",
    "            print(\"\\rTesting {}/{} {:.1f}%\".format(it, n_iter_test_per_epoch, 100.0 * it / n_iter_test_per_epoch), end=\" \"*30)\t\n",
    "\n",
    "        loss_test = np.mean(loss_test_ep)\n",
    "        acc_test  = np.mean(acc_test_ep)\n",
    "\n",
    "        print(\"\\r(Testing) accuracy: {:.3f}%, loss: {:.4f}\".format(acc_test*100.0, loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from tmp/model.ckpt\n",
      "\n",
      "\n",
      "(Testing) accuracy: 99.600%, loss: 0.0071           \n"
     ]
    }
   ],
   "source": [
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction(model, num_samples):\n",
    "    samples_imgs = mnist.test.images[:num_samples].reshape([-1, 28, 28, 1])\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.import_meta_graph(checkpoint_file +'.meta')\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('tmp/'))\n",
    "\n",
    "        decoder_output, y_pred_value = sess.run(\n",
    "            [model.decoder_output, model.y_pred],\n",
    "            feed_dict = {model.X_cropped: samples_imgs})\n",
    "\n",
    "\n",
    "    samples_imgs = samples_imgs.reshape([-1, 28, 28])\n",
    "    reconstructions_imgs = decoder_output.reshape([-1, 28, 28])\t\n",
    "\n",
    "    plt.figure(figsize = (num_samples * 2, 4))\n",
    "\n",
    "    for img_idx in range(num_samples):\n",
    "        plt.subplot(2, num_samples, img_idx + 1)\n",
    "        plt.imshow(samples_imgs[img_idx], cmap='gray')\n",
    "        plt.title(\"Input: \" + str(mnist.test.labels[img_idx]))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    for img_idx in range(num_samples):\n",
    "        plt.subplot(2, num_samples, num_samples + img_idx + 1)\n",
    "        plt.imshow(reconstructions_imgs[img_idx], cmap='gray')\n",
    "        plt.title(\"Output: \" + str(y_pred_value[img_idx]))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reconstruction(model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
