{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from capsnet import CapsNet\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/')\n",
    "batch_size = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "tf.random.set_random_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "checkpoint_file = './tmp/model.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def count_params():\n",
    "    size = lambda v: functools.reduce(lambda x, y: x*y, v.get_shape().as_list())\n",
    "    n_trainable = sum(size(v) for v in tf.trainable_variables())\n",
    "    print(\"Model size (Trainable): {:.1f}M\\n\".format(n_trainable/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, restore = False, n_epochs = 50):\n",
    "    init = tf.global_variables_initializer()\t\n",
    "\n",
    "    n_iter_train_per_epoch = mnist.train.num_examples // batch_size\n",
    "    n_iter_valid_per_epoch = mnist.validation.num_examples // batch_size\n",
    "    n_iter_test_per_epoch  = mnist.test.num_examples // batch_size\n",
    "\n",
    "    best_loss_val = np.infty\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(\"output\", sess.graph)\n",
    "\n",
    "        if restore and tf.train.checkpoint_exists('checkpoint_file'):\n",
    "            saver.restore(sess, checkpoint_file)\n",
    "        else:\n",
    "            init.run()\n",
    "\n",
    "        print('\\n\\nRunning CapsNet ...\\n')\n",
    "        count_params()\n",
    "\n",
    "        print(\"\\ntr_loss   : training loss(margin loss)\")\n",
    "        print(\"val_loss  : validation loss\")\n",
    "        print(\"train_acc : training accuracy(%)\")\n",
    "        print(\"val_acc   : validation accuracy(%)\\n\")\n",
    "        \n",
    "        recnst_loss_train = 0.0\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            loss_train_ep = []\n",
    "            acc_train_ep  = []\n",
    "            for it in range(1, n_iter_train_per_epoch + 1):\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "                \n",
    "                _, loss_batch_train, acc_batch_train = sess.run(\n",
    "                                [model.train_op, \n",
    "                                 model.batch_loss,\n",
    "                                 model.accuracy],\n",
    "                                feed_dict = {model.X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                                                model.y: y_batch})\n",
    "\n",
    "                print(\"\\rIter: {}/{} [{:.1f}%] loss : {:.5f}\".format(\n",
    "                    it, n_iter_train_per_epoch, 100.0 * it / n_iter_train_per_epoch, loss_batch_train), end=\"\")\n",
    "\n",
    "                loss_train_ep.append(loss_batch_train)\n",
    "                acc_train_ep.append(acc_batch_train)\n",
    "                \n",
    "            loss_train = np.mean(loss_train_ep)\n",
    "            acc_train = np.mean(acc_train_ep)\n",
    "            \n",
    "            loss_val_ep = []\n",
    "            acc_val_ep  = []\n",
    "\n",
    "            for it in range(1, n_iter_valid_per_epoch + 1):\n",
    "                X_batch, y_batch = mnist.validation.next_batch(batch_size)\n",
    "                loss_batch_val, acc_batch_val = sess.run(\n",
    "                                [model.batch_loss, model.accuracy],\n",
    "                                feed_dict = {model.X_cropped: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                                                model.y: y_batch})\n",
    "\n",
    "                loss_val_ep.append(loss_batch_val)\n",
    "                acc_val_ep.append(acc_batch_val)\n",
    "\n",
    "                print(\"\\rValidation {}/{} {:.1f}%\".format(it, n_iter_valid_per_epoch, 100.0 * it / n_iter_valid_per_epoch), end=\" \"*30)\n",
    "\n",
    "            loss_val = np.mean(loss_val_ep)\n",
    "            acc_val  = np.mean(acc_val_ep)\n",
    "            \n",
    "            print(\"\\rEp {:2d}: train_loss:{:.4f}, valid_loss:{:.4f}, train_acc:{:.3f}, val_acc:{:.2f}\".format(\n",
    "                epoch + 1, \n",
    "                loss_train,\n",
    "                loss_val, \n",
    "                acc_train * 100.0, \n",
    "                acc_val * 100.0))\n",
    "\n",
    "            saver.save(sess, checkpoint_file)\n",
    "            \n",
    "\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CapsNet(rounds = 3, batch_size = batch_size,reconstruction_net = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running CapsNet ...\n",
      "\n",
      "Model size (Trainable): 6.8M\n",
      "\n",
      "\n",
      "tr_loss   : training loss(margin loss)\n",
      "val_loss  : validation loss\n",
      "train_acc : training accuracy(%)\n",
      "val_acc   : validation accuracy(%)\n",
      "\n",
      "Ep  1: train_loss:0.1494, valid_loss:0.0253, train_acc:83.616, val_acc:97.92\n",
      "Ep  2: train_loss:0.0270, valid_loss:0.0138, train_acc:97.867, val_acc:98.94\n",
      "Ep  3: train_loss:0.0179, valid_loss:0.0120, train_acc:98.605, val_acc:99.10\n",
      "Ep  4: train_loss:0.0136, valid_loss:0.0091, train_acc:98.951, val_acc:99.26\n",
      "Ep  5: train_loss:0.0111, valid_loss:0.0081, train_acc:99.124, val_acc:99.38\n",
      "Ep  6: train_loss:0.0098, valid_loss:0.0082, train_acc:99.227, val_acc:99.28\n",
      "Ep  7: train_loss:0.0082, valid_loss:0.0070, train_acc:99.389, val_acc:99.38\n",
      "Ep  8: train_loss:0.0073, valid_loss:0.0067, train_acc:99.444, val_acc:99.44\n",
      "Ep  9: train_loss:0.0069, valid_loss:0.0062, train_acc:99.496, val_acc:99.46\n",
      "Ep 10: train_loss:0.0061, valid_loss:0.0055, train_acc:99.551, val_acc:99.50\n",
      "Ep 11: train_loss:0.0057, valid_loss:0.0061, train_acc:99.589, val_acc:99.42\n",
      "Ep 12: train_loss:0.0049, valid_loss:0.0056, train_acc:99.647, val_acc:99.50\n",
      "Ep 13: train_loss:0.0046, valid_loss:0.0054, train_acc:99.671, val_acc:99.52\n",
      "Ep 14: train_loss:0.0041, valid_loss:0.0047, train_acc:99.720, val_acc:99.54\n",
      "Ep 15: train_loss:0.0040, valid_loss:0.0050, train_acc:99.731, val_acc:99.52\n",
      "Ep 16: train_loss:0.0036, valid_loss:0.0046, train_acc:99.749, val_acc:99.58\n",
      "Ep 17: train_loss:0.0035, valid_loss:0.0046, train_acc:99.756, val_acc:99.54\n",
      "Ep 18: train_loss:0.0032, valid_loss:0.0045, train_acc:99.776, val_acc:99.54\n",
      "Ep 19: train_loss:0.0027, valid_loss:0.0044, train_acc:99.824, val_acc:99.56\n",
      "Ep 20: train_loss:0.0027, valid_loss:0.0045, train_acc:99.815, val_acc:99.58\n",
      "Ep 21: train_loss:0.0026, valid_loss:0.0043, train_acc:99.860, val_acc:99.50\n",
      "Ep 22: train_loss:0.0024, valid_loss:0.0042, train_acc:99.847, val_acc:99.56\n",
      "Ep 23: train_loss:0.0023, valid_loss:0.0042, train_acc:99.862, val_acc:99.54\n",
      "Ep 24: train_loss:0.0021, valid_loss:0.0042, train_acc:99.880, val_acc:99.58\n",
      "Ep 25: train_loss:0.0019, valid_loss:0.0043, train_acc:99.887, val_acc:99.58\n",
      "Ep 26: train_loss:0.0019, valid_loss:0.0041, train_acc:99.880, val_acc:99.62\n",
      "Ep 27: train_loss:0.0018, valid_loss:0.0039, train_acc:99.887, val_acc:99.70\n",
      "Ep 28: train_loss:0.0018, valid_loss:0.0041, train_acc:99.904, val_acc:99.62\n",
      "Ep 29: train_loss:0.0017, valid_loss:0.0042, train_acc:99.893, val_acc:99.58\n",
      "Ep 30: train_loss:0.0016, valid_loss:0.0040, train_acc:99.905, val_acc:99.62\n",
      "Ep 31: train_loss:0.0016, valid_loss:0.0040, train_acc:99.900, val_acc:99.62\n",
      "Ep 32: train_loss:0.0016, valid_loss:0.0039, train_acc:99.913, val_acc:99.64\n",
      "Ep 33: train_loss:0.0015, valid_loss:0.0040, train_acc:99.911, val_acc:99.62\n",
      "Ep 34: train_loss:0.0015, valid_loss:0.0040, train_acc:99.929, val_acc:99.58\n",
      "Ep 35: train_loss:0.0014, valid_loss:0.0039, train_acc:99.924, val_acc:99.62\n",
      "Ep 36: train_loss:0.0014, valid_loss:0.0039, train_acc:99.924, val_acc:99.64\n",
      "Ep 37: train_loss:0.0013, valid_loss:0.0039, train_acc:99.925, val_acc:99.66\n",
      "Ep 38: train_loss:0.0014, valid_loss:0.0039, train_acc:99.929, val_acc:99.60\n",
      "Ep 39: train_loss:0.0013, valid_loss:0.0039, train_acc:99.929, val_acc:99.62\n",
      "Ep 40: train_loss:0.0012, valid_loss:0.0039, train_acc:99.945, val_acc:99.62\n",
      "Ep 41: train_loss:0.0012, valid_loss:0.0039, train_acc:99.929, val_acc:99.62\n",
      "Ep 42: train_loss:0.0012, valid_loss:0.0039, train_acc:99.935, val_acc:99.62\n",
      "Ep 43: train_loss:0.0013, valid_loss:0.0039, train_acc:99.935, val_acc:99.60\n",
      "Ep 44: train_loss:0.0012, valid_loss:0.0039, train_acc:99.931, val_acc:99.60\n",
      "Ep 45: train_loss:0.0012, valid_loss:0.0039, train_acc:99.925, val_acc:99.64\n",
      "Ep 46: train_loss:0.0012, valid_loss:0.0039, train_acc:99.931, val_acc:99.64\n",
      "Ep 47: train_loss:0.0012, valid_loss:0.0039, train_acc:99.927, val_acc:99.60\n",
      "Ep 48: train_loss:0.0012, valid_loss:0.0039, train_acc:99.936, val_acc:99.60\n",
      "Ep 49: train_loss:0.0012, valid_loss:0.0039, train_acc:99.933, val_acc:99.64\n",
      "Ep 50: train_loss:0.0011, valid_loss:0.0039, train_acc:99.945, val_acc:99.60\n"
     ]
    }
   ],
   "source": [
    "train(model, False, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    n_iter_test_per_epoch = mnist.test.num_examples // batch_size\n",
    "\n",
    "    loss_test_ep = []\n",
    "    acc_test_ep  = []\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('tmp/'))\n",
    "        print('\\n\\nTest\\n')\n",
    "        \n",
    "        for it in range(1, n_iter_test_per_epoch + 1):\n",
    "            X_batch, y_batch = mnist.test.next_batch(batch_size)\n",
    "            loss_batch_test, acc_batch_test = sess.run(\n",
    "                                [model.batch_loss, model.accuracy],\n",
    "                                feed_dict = { model.X_cropped: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                                    model.y: y_batch})\n",
    "                                    #model.reconstruction: False})\n",
    "\n",
    "            loss_test_ep.append(loss_batch_test)\n",
    "            acc_test_ep.append(acc_batch_test)\n",
    "            print(\"\\rTesting {}/{} {:.1f}%\".format(it,\n",
    "                                            n_iter_test_per_epoch,\n",
    "                                            100.0 * it / n_iter_test_per_epoch),\n",
    "                                            end=\" \"*30)\n",
    "\n",
    "        loss_test = np.mean(loss_test_ep)\n",
    "        acc_test  = np.mean(acc_test_ep)\n",
    "\n",
    "        print(\"\\r(Testing) accuracy: {:.3f}%, loss: {:.4f}\".format(acc_test*100.0, loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from tmp/model.ckpt\n",
      "\n",
      "\n",
      "Test\n",
      "\n",
      "(Testing) accuracy: 99.650%, loss: 0.0042         \n"
     ]
    }
   ],
   "source": [
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
